---
title: "Confidence and Bias -- Analysis"
author: "Timothy J. Luke"
date: "`r Sys.Date()`"
output: 
  html_document:
    theme: darkly
    toc: true
    toc_float: true
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(echo = TRUE)

source("./R/wrangle.R")
source("./R/analyze.R")

```

# Does sender confidence influence bias?

## Raw judgments

To assess whether sender confidence (a manipulated variable) increases truth bias, we conducted a mixed effects logistic regression analysis. We began with a model in which veracity judgments (0 = lie, 1 = truth) were predicted by actual sender veracity (lie vs. truth) and by message content (holiday, bereavement, accident, and quarrel; dummy coded with holiday as the reference group), as well as random intercepts for each receiver and each sender. We then fit a model that added confidence condition and a third model that added the interaction term between content and confidence condition. Then we compared the models to select one using a likelihood ratio test. As can be seen below, the test suggested better fit with the addition of the interaction term.

```{r}
lrt_judge_fixed
```

Actual veracity was not a significant predictor of judged veracity. 

The coefficient for confidence condition is positive and significant, suggesting that sender confidence makes receivers more likely to judge messages as truthful, for the holiday messages. The coefficients for the interactions suggest that sender confidence had different effects depending on the message content. For bereavement messages, the effect was similar to the effect for holiday messages. For quarrels, high sender confidence seemed to decrease the rate of truth judgments. For accidents, high sender confidence did not seem to have much of an effect (note that the interaction coefficient is significant, but the sum of the relevant coefficients suggests only a small difference in effects). That is, for accidents, high confidence had less of an impact on truth judgments than it did for holiday messages.

Additionally, there is some variance in the tendency for different senders to be judged as telling the truth. There is less -- but still some -- variance in receivers' tendency to judge messages as truthful.

```{r}
summary(judge_fixed_conf_int)
```

### Mean predicted truth judgment rates

For a more intuitive examination of the results of the model above, we can extract predicted truth judgment rates and calculate means for each condition, sender, and content type. For each mean predicted rate of truth judgments, I computed 95% parametric bootstrap confidence intervals using 500 simulations.

First, we can examine the difference in truth judgment rates by condition. Below, the rates are displayed as proportions of truth judgments (i.e., values closer to 1 indicate higher truth bias). We can see that the high confidence condition is predicted to be judged more truthful more frequently, by about 5%.

```{r}
boot_ci_fixed_cond
```

As we saw above, there is substantial variation in the rate at which different senders are predicted to be judged as telling the truth.

```{r}
boot_ci_fixed_sender
```

We can also see that the mean predicted truth judgment rates vary substantially between the content types. Bereavement and holidays are predicted to be judged truthful the most frequently, and interestingly, quarrels are predicted to be judged deceptive a narrow majority of the time. Car accidents are predicted to be judged truthful narrowly above 50% of the time, but judged truthful less frequently than bereavements and holidays.

```{r}
boot_ci_fixed_cont
```

We can examine the interaction between confidence condition and message content as well. We can see that, as suggested by the model above, there are similar effects of sender confidence for holidays and bereavement messages. The rate of truth judgments for accidents does not change much. If anything, it increases by just a few percentage points, but the effect (if it is present at all) is more muted compared to holidays and bereavement messages. Quarrels seem to become somewhat less credible when delivered with high confidence. 

```{r}
boot_ci_fixed_inter
```

In short, we see that sender confidence appears to influence veracity judgments (or truth bias). This effect depends on the content of the message, and the specific sender also exerts substantial influence on veracity judgments.

```{r}
boot_ci_fixed_inter_2
```

## Signal detection

As a robustness check, I assessed whether confidence condition predicted the signal detection index "c" -- a measure of bias. To perform this check, after calculating c for each receiver (collapsed across the content types), I fit a linear regression model predicting c from confidence condition. As can be seen below, the results of this analysis support the results of the models examining raw judgments.

```{r}
summary(sdt_model_base)
```

We think the analyses of raw judgments are more informative (and more intuitive) than the signal detection analysis. But it's nice to see this kind of consistency.

## Message content as a random effect

Another way of analyzing these data is to model message content as a random effect, rather than a fixed effect (as above). This approach would imply that the four content types represent a random sample from a population of possible narrative types. Whether this approach is more or less appropriate than the fixed effect approach taken above can be debated. However, the results are substantively the same as the fixed effect approach presented above.

We start by comparing three models: (1) an initial model with actual sender veracity as a predictor of veracity judgments, with random intercepts for receivers and for senders nested in content types, (2) a model which adds confidence condition as a predictor, and (3) a model that adds random slopes for confidence condition for each content type (which represent an interaction between content and confidence). We compared these models with a likelihood ratio test.

As above, the model implying an interaction between content and confidence appears to be the best fitting of the three.

```{r}
lrt_judge
```

The model with random slopes fits the data the best, even though none of the fixed effects are significant (see below). In the model with only random intercepts, we see that there is a tendency for sender confidence to increase truth judgments on average, but adding the interaction substantially increases the standard error for this coefficient. Thus, we see evidence for an interaction between content type and confidence condition, such that sender confidence has effects of different sizes and/or directions depending on the content of the message. 

These models are consistent with the fixed content effect approach above, as well as the signal detection model.

```{r}
summary(judge_model_conf)
```

```{r}
summary(judge_model_conf_int)
```

### Mean predicted truth judgment rates

As above, for a more intuitive examination of the results of the model above, we can use these models to extract predicted truth judgment rates and calculate means for each condition, sender, and content type. Again, for each mean predicted rate of truth judgments, I computed 95% parametric bootstrap confidence intervals using 500 simulations.

These predicted rates of truth judgments are highly consistent with the approach presented above.

```{r}
boot_ci_cond
```

```{r}
boot_ci_sender
```

```{r}
boot_ci_cont
```

```{r}
boot_ci_inter
```

### Raw veracity judgment rates

Here are the overall veracity judgment rates. People were somewhat truth-biased.

```{r}
table(judge_long$judgment)/sum(table(judge_long$judgment))
```

People were more truth-biased when the sender was more confident, across the
content types.

```{r}
judge_long %>% 
  group_by(confidence_condition) %>% 
  summarise(
    truth_rate = sum(judgment == 1) / n()
  ) %>% 
  knitr::kable()
```

```{r}
judge_long %>% 
  group_by(content) %>% 
  summarise(
    truth_rate = sum(judgment == 1) / n()
  ) %>% 
  knitr::kable()
```

```{r}
judge_long %>% 
  group_by(content, confidence_condition) %>% 
  summarise(
    truth_rate = sum(judgment == 1) / n()
  ) %>% 
  knitr::kable()
```

```{r}
judge_long %>% 
  group_by(content, confidence_condition, veracity) %>% 
  summarise(
    truth_rate = sum(judgment == 1) / n()
  ) %>% 
  knitr::kable()
```

# Does sender confidence influence receiver accuracy?

## Raw judgments

To assess whether sender confidence influenced receiver accuracy, we conducted a mixed effects logistic regression analysis. As with the analysis of truth judgments, we began with a model in which judgment accuracy (0 = incorrect, 1 = correct) were predicted by actual sender veracity (lie vs. truth) and by message content (holiday, bereavement, accident, and quarrel; dummy coded with holiday as the reference group), as well as random intercepts for each receiver and each sender. We then fit a model that added confidence condition and a third model that added the interaction term between content and confidence condition. Then we compared the models to select one using a likelihood ratio test. As can be seen below, the test suggested the subsequent models after the initial model offered no significant improvement.

```{r}
lrt_acc
```

Given that the model that added sender confidence as a predictor did not improve
the model fit, there was no evidence that sender confidence predicted accuracy. 
Actual sender veracity predicted accuracy, such that receivers were more 
accurate judging truthful messages. This is unsurprising given the truth bias 
that receivers exhibited. There was also no indication that the different content
 types varied with regard to receiver accuracy.

Interestingly but consistent with past research, we see that there was virtually
 no variance in accuracy associated with receivers, but there was a substantial 
amount of variance associated with senders.

```{r}
summary(acc_model_base)
```

### Accuracy rates

It can be informative to examine the raw accuracy rates across and between
conditions.

The overall accuracy rate was approximately `r round((table(accuracy_long$accuracy)/sum(table(accuracy_long$accuracy)))[2], 3) * 100`%.

```{r}
table(accuracy_long$accuracy)/sum(table(accuracy_long$accuracy))
```

The effect of the truth bias on accuracy is evident when splitting accuracy by
veracity condition.

```{r}
accuracy_long %>% 
  group_by(veracity) %>% 
  summarise(
    acc_rate = sum(accuracy == 1) / n()
  ) %>% 
  knitr::kable()
```

Overall accuracy between the content types varied slightly, but the model above 
suggests that these differences were not estimated precisely enough for us to 
conclude that they are reliable.

```{r}
accuracy_long %>% 
  group_by(content) %>% 
  summarise(
    acc_rate = sum(accuracy == 1) / n()
  ) %>% 
  knitr::kable()
```

Differences across content in the extent to which truth bias influenced accuracy 
are what you would expect given the differences in truth bias across content.
That is, when there is greater truth bias, there is better accuracy for truthful
messages. No surprises there.

```{r}
accuracy_long %>% 
  group_by(content, veracity) %>% 
  summarise(
    acc_rate = sum(accuracy == 1) / n()
  ) %>% 
  knitr::kable()
```

```{r}
accuracy_long %>% 
  group_by(content, confidence_condition, veracity) %>% 
  summarise(
    acc_rate = sum(accuracy == 1) / n()
  ) %>% 
  knitr::kable()
```


## Signal detection

Consistent with the analysis of the raw judgments, when we fit a linear model to
predict d' (a measure of discrimination) with sender confidence as a predictor, 
we see no evidence that sender confidence influences receiver accuracy.

```{r}
summary(sdt_model_acc)
```

# Does confidence predict accuracy?

To assess the extent to which confidence predicted accuracy (viz. are correct
judgments made more confidently?), we fit an additional logistic mixed effects 
model predicting accuracy with sender veracity, content type, and receiver 
confidence. We compared this model to the retained model predicting accuracy 
above. Adding recevier confidence offered no significant improvement to the 
model.

```{r}
lrt_confidence
```

For the sake of completeness, here is the main model output. As can be seen,
confidence did not appear to predict accuracy at all.

```{r}
summary(acc_conf_model_base)
```

